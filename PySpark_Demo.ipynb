{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Retention Demo Using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will show PySpark functionality accessing enterprise data from VSAM and DB2. The data stored in VSAM consists of 6,001 rows of customer information. The data stored in DB2 consists of 20,000 rows of transaction data. The data is transformed and joined within a Spark dataframe, which is used to perform predictive analyses. A logistic regression algorithm is then used to evaluate cutomer activity level vs. churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)\n",
    "\n",
    "zOS_IP = \"123.456.78.901\"\n",
    "MDSS_PORT = \"1200\"\n",
    "zOS_USERNAME = \"SPK????\"\n",
    "zOS_PASSWORD = \"????????\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession is the entry point to using Spark on z/OS APIs through IBM Open Data Analytics for z/OS (IzODA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"spark://\"+zOS_IP+\":7077\").appName(\"pyspark_demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Client Data***\n",
    "\n",
    "Load client data into a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jdbc_connection = \"jdbc:rs:dv://\"+zOS_IP+\":\"+MDSS_PORT+\"; DBTY=DVS; SUBSYS=NONE; UID=\"+zOS_USERNAME+\"; PWD=\"+zOS_PASSWORD+\";\"\n",
    "raw_clientInfo_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_connection) \\\n",
    "        .option(\"dbtable\", \"VSAM_CLIENT\") \\\n",
    "        .load()\n",
    "clientInfo_df = raw_clientInfo_df. \\\n",
    "    toDF(\"customer_id\",\"gender\",\"age_years\",\"highest_edu\",\"annual_investment_rev\",\"annual_income\",\"activity_level\",\"churn\", \"rid\"). \\\n",
    "    select(\"customer_id\",\"gender\",\"age_years\",\"highest_edu\",\"annual_investment_rev\",\"annual_income\",\"activity_level\",\"churn\")\n",
    "        \n",
    "clientInfo_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Credit card transactions***\n",
    "\n",
    "Load credit card transactions into a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DB2_SSID = \"DBBG\"\n",
    "jdbc_connection = \"jdbc:rs:dv://\"+zOS_IP+\":\"+MDSS_PORT+\"; DBTY=DB2; SUBSYS=\"+DB2_SSID+\"; UID=\"+zOS_USERNAME+\"; PWD=\"+zOS_PASSWORD+\";\"\n",
    "clientTrans_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_connection) \\\n",
    "        .option(\"dbtable\", \"sparkdb.sppaytb1\") \\\n",
    "        .load()\n",
    "clientTrans_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate statistics\n",
    "Calculate a few aggregate statistics based on credit transactions and join the results to the client data dataframe. We also convert some of the fields to integer and double type, which will be used in following calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "calcTrans_df = clientTrans_df.groupBy(\"CONT_ID\").agg(func.sum(\"ACAUREQ_AUREQ_TX_DT_TTLAMT\"), \\\n",
    "func.count(\"ACAUREQ_AUREQ_TX_DT_TTLAMT\")/365, \\\n",
    "func.count(\"ACAUREQ_AUREQ_TX_DT_TTLAMT\"), \\\n",
    "func.sum(\"ACAUREQ_AUREQ_TX_DT_TTLAMT\")/func.count(\"ACAUREQ_AUREQ_TX_DT_TTLAMT\"))\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"CONT_ID\", IntegerType()), \\\n",
    "    StructField(\"total_txn_amount\", DoubleType()), \\\n",
    "    StructField(\"avg_daily_txns\", DoubleType()), \\\n",
    "    StructField(\"total_txns\", IntegerType()), \\\n",
    "    StructField(\"avg_txn_amount\", DoubleType()) \\\n",
    "    ])\n",
    "    \n",
    "calcTrans_df = calcTrans_df.toDF('CONT_ID', 'total_txn_amount', 'avg_daily_txns', 'total_txns', 'avg_txn_amount')\n",
    "calcTrans_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client_df = clientInfo_df.join(calcTrans_df, calcTrans_df.CONT_ID == clientInfo_df.customer_id, \"inner\")\n",
    "client_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Analyses\n",
    "\n",
    "We now start to do some predictive analyses on the data to evaluate cutomer activity level vs. churn. We use a supervised learning algorithm, logistic regression, to train the model. Logistic regression is a common, fast, highly scalable, classification model that doesn't require much tuning and is easy to regularize. The model outputs a set of probabilities which can be more useful than class labels. Here, we will use PySpark Machine Learning library to create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "input_df = client_df\n",
    "formula = RFormula().setFormula(\"churn ~ activity_level\").setFeaturesCol(\"features\").setLabelCol(\"label\")\n",
    "train_df = formula.fit(input_df).transform(input_df).select(\"label\", \"features\")\n",
    "\n",
    "model =  LogisticRegression().setThreshold(0.5)\n",
    "result_df = model.fit(train_df).transform(train_df)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the S-curve for cutomer activity level vs. churn. Matplotlib and Seaborn are two common plotting libraries used in Python. These plotting libraries are useful in creating custom visualizations to help gain insights from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "plot_list = [(e['features'].item(0),1-e['probability'].item(0)) for e in result_df.collect()]\n",
    "X = [plot_pair[0] for plot_pair in plot_list]\n",
    "Y = [plot_pair[1] for plot_pair in plot_list]\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Activity Level\", fontsize=16)\n",
    "plt.ylabel(\"Probability of Churn\", fontsize=16)\n",
    "sns.regplot(x=np.array(X),y=np.array(Y), order=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
